@Article{he2021automl,
  author    = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  journal   = {Knowledge-Based Systems},
  title     = {AutoML: A survey of the state-of-the-art},
  year      = {2021},
  pages     = {106622},
  volume    = {212},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/he-xin-automl-a-survey-of-the-state-of-the-art.pdf:PDF},
  publisher = {Elsevier},
}

@Book{hutter2019automated,
  author    = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  publisher = {Springer Nature},
  title     = {Automated machine learning: methods, systems, challenges},
  year      = {2019},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/Frank Hutter Automated Machine Learning.pdf:PDF},
}

@Article{zoller2021benchmark,
  author  = {Z{\"o}ller, Marc-Andr{\'e} and Huber, Marco F},
  journal = {Journal of artificial intelligence research},
  title   = {Benchmark and survey of automated machine learning frameworks},
  year    = {2021},
  pages   = {409--472},
  volume  = {70},
  file    = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/zöller-marc-andré-benchmark-and-survey-of-automated.pdf:PDF},
}

@InProceedings{tuggener2019automated,
  author       = {Tuggener, Lukas and Amirian, Mohammadreza and Rombach, Katharina and L{\"o}rwald, Stefan and Varlet, Anastasia and Westermann, Christian and Stadelmann, Thilo},
  booktitle    = {2019 6th Swiss Conference on Data Science (SDS)},
  title        = {Automated machine learning in practice: state of the art and recent results},
  year         = {2019},
  organization = {IEEE},
  pages        = {31--36},
  file         = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/tuggener2019.pdf:PDF},
}

@Article{moreno2013comparative,
  author    = {Moreno, Alberto Prieto and Santiago, Orestes Llanes and de Lazaro, Jose Manuel Bernal and Moreno, Emilio Garcia},
  journal   = {IEEE Latin America Transactions},
  title     = {Comparative evaluation of classification methods used in fault diagnosis of industrial processes},
  year      = {2013},
  number    = {2},
  pages     = {682--689},
  volume    = {11},
  abstract  = {This article presents a comparative study of the obtención de los modelos adecuados, se usan otros enfoques
performance of classification techniques used for fault diagnosis como son la aplicación de técnicas de inteligencia artificial 
in industrial processes. The techniques studied ranging from para capturar el conocimiento experto y la utilización de 
classifiers based on Bayes theory as Maximum a Posteriori 
Probability (MAP) and Nearest Neighbor (kNN) classifiers, técnicas estadísticas multivariadas y de reconocimiento de},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/prietomoreno2013.pdf:PDF},
  keywords  = {industrial processes, fault diagnosis, support vector que las cinco herramientas de clasificación más difundidas machines, artificial neural networks, partial least squares, son: el clasificador de Máxima Probabilidad a Posteriori nearest neighbors classifier, MAP classifier},
  publisher = {IEEE},
}

@Book{sammut2011encyclopedia,
  author    = {Sammut, Claude and Webb, Geoffrey I},
  publisher = {Springer Science \& Business Media},
  title     = {Encyclopedia of machine learning},
  year      = {2011},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/sammut-claude-encyclopedia-of-machine-learning-2010.pdf:PDF},
}

@InProceedings{Feurer2019,
  author   = {Matthias Feurer and Aaron Klein and Katharina Eggensperger and Jost Tobias Springenberg and Manuel Blum and Frank Hutter},
  title    = {Chapter 6 Auto-sklearn: Efficient and Robust Automated Machine Learning},
  year     = {2019},
  abstract = {The success of machine learning in a broad range of applications has led
to an ever-growing demand for machine learning systems that can be used off the
shelf by non-experts. To be effective in practice, such systems need to automatically
choose a good algorithm and feature preprocessing steps for a new dataset at hand,
and also set their respective hyperparameters. Recent work has started to tackle this
automated machine learning (AutoML) problem with the help of efficient Bayesian
optimization methods. Building on this, we introduce a robust new AutoML system
based on the Python machine learning package scikit-learn (using 15 classifiers, 14
feature preprocessing methods, and 4 data preprocessing methods, giving rise to a
structured hypothesis space with 110 hyperparameters). This system, which we dub
Auto-sklearn, improves on existing AutoML methods by automatically taking into
account past performance on similar datasets, and by constructing ensembles from
the models evaluated during the optimization. Our system won six out of ten phases
of the first ChaLearn AutoML challenge, and our comprehensive analysis on over
100 diverse datasets shows that it substantially outperforms the previous state of
the art in AutoML. We also demonstrate the performance gains due to each of our
contributions and derive insights into the effectiveness of the individual components
of Auto-sklearn.},
  doi      = {.org/10.1007/978-3-030-05318-5_6},
  file     = {:Feurer2019 - Chapter 6 Auto Sklearn_ Efficient and Robust Automated Machine Learning.pdf:PDF;:liu1-2.pdf:PDF},
}

@Article{waring2020automated,
  author    = {Waring, Jonathan and Lindvall, Charlotta and Umeton, Renato},
  journal   = {Artificial intelligence in medicine},
  title     = {Automated machine learning: Review of the state-of-the-art and opportunities for healthcare},
  year      = {2020},
  pages     = {101822},
  volume    = {104},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/waring-jonathan-automated-machine-learning-review-of.pdf:PDF},
  publisher = {Elsevier},
}

@Book{orallo2004,
  author    = {Hern{\'a}ndez Orallo, Jos{\'e} and others},
  publisher = {Biblioteca Hern{\'a}n Malo Gonz{\'a}lez},
  title     = {Introducci{\'o}n a la Miner{\'\i}a de Datos},
  year      = {2004},
  file      = {:D\:/School/Informatica/3er Año/1er trimestre/Practicas/Data mining/curso mineria de datos y knime/bibliografía/Libro Introducción a la MD (Hernández Orallo).pdf:PDF},
}

@InProceedings{Agrawal1519,
  author   = {Rakesh Agrawal and Tomasz Imielinski and Arun Swami},
  title    = {Mining Association Rules between Sets of Items in Large Databases},
  year     = {1993},
  abstract = {on tertiary storage and are very slowly migrating to},
  file     = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/agrawal1993.pdf:PDF},
}

@InProceedings{moine2011estudio,
  author    = {Moine, Juan Miguel and Haedo, Ana Silvia and Gordillo, Silvia Ethel},
  booktitle = {XIII Workshop de Investigadores en Ciencias de la Computaci{\'o}n},
  title     = {Estudio comparativo de metodolog{\'\i}as para miner{\'\i}a de datos},
  year      = {2011},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/Estudio comparativo de metodologías para minería de datos..pdf:PDF},
}

@InProceedings{Han1999,
  author = {Han, Jiawei.; Kamber, Micheline.},
  title  = {Data Mining : Concepts and Techniques},
  year   = {1999},
  file   = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/data-mining-concepts-and-techniques-2nd-edition-impressao.pdf:PDF},
}

@Book{murphy2012machine,
  author    = {Murphy, Kevin P},
  publisher = {MIT press},
  title     = {Machine learning: a probabilistic perspective},
  year      = {2012},
  file      = {:murphy2012machine - Machine Learning_ a Probabilistic Perspective.pdf:PDF;:Data Preprocessing in Data Mining.pdf:PDF},
}

@Book{garcia2015data,
  author    = {Garc{\'\i}a, Salvador and Luengo, Juli{\'a}n and Herrera, Francisco},
  publisher = {Springer},
  title     = {Data preprocessing in data mining},
  year      = {2015},
  file      = {:garcia2015data - Data Preprocessing in Data Mining.pdf:PDF;:ACA-22-67.pdf:PDF},
}

@InProceedings{Carrazana2022,
  author = {Carrazana Ruiz, Ernesto},
  title  = {Componente KNIME de AutoML para pre-procesado en tareas de Clasificación},
  year   = {2022},
  file   = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/cosas de ernesto/Tesis_Ernesto_Carrazana_Ruiz_IC/Tesis_Ernesto_Carrazana_Ruiz_IC/Tesis_Ernesto_Carrazana_Ruiz_IC.pdf:PDF},
}

@Article{celik2022online,
  author    = {Celik, Bilge and Singh, Prabhant and Vanschoren, Joaquin},
  journal   = {Machine Learning},
  title     = {Online AutoML: An adaptive AutoML framework for online learning},
  year      = {2022},
  pages     = {1--25},
  abstract  = {Automated Machine Learning (AutoML) has been used successfully in settings where 
the learning task is assumed to be static. In many real-world scenarios, however, the data 
distribution will evolve over time, and it is yet to be shown whether AutoML techniques 
can effectively design online pipelines in dynamic environments. This study aims to auto-
mate pipeline design for online learning while continuously adapting to data drift. For this 
purpose, we design an adaptive Online Automated Machine Learning (OAML) system, 
searching the complete pipeline configuration space of online learners, including preproc-
essing algorithms and ensembling techniques. This system combines the inherent adapta-
tion capabilities of online learners with fast automated pipeline (re)optimization. Focusing 
on optimization techniques that can adapt to evolving objectives, we evaluate asynchronous 
genetic programming and asynchronous successive halving to optimize these pipelines 
continually. We experiment on real and artificial data streams with varying types of con-
cept drift to test the performance and adaptation capabilities of the proposed system. The 
results confirm the utility of OAML over popular online learning algorithms and under-
score the benefits of continuous pipeline redesign in the presence of data drift.},
  doi       = {.org/10.1007/s10994-022-06262-0},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/celik-bilge-online-automl-an-adaptive-automl-framework.pdf:PDF},
  keywords  = {Online automl · Automated online learning · Concept drift · Automated drift adaptation},
  publisher = {Springer},
}

@Article{soh2020introduction,
  author    = {Soh, Julian and Singh, Priyanshi and Soh, Julian and Singh, Priyanshi},
  journal   = {Data Science Solutions on Azure: Tools and Techniques Using Databricks and MLOps},
  title     = {Introduction to azure machine learning},
  year      = {2020},
  pages     = {117--148},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/soh-julian-introduction-to-azure-machine-learning-2020.pdf:PDF},
  publisher = {Springer},
}

@InProceedings{Das2020,
  author   = {Piali Das and Nikita Ivkin and Tanya Bansal and Laurence Rouesnel and Philip Gautier and Zohar Karnin and Leo Dirac and Lakshmi Ramakrishnan and Andre Perunicic and Iaroslav Shcherbatyi and Wilton Wu},
  title    = {Amazon SageMaker Autopilot: a white box AutoML solution at scale},
  year     = {2020},
  abstract = {data to be prepared and transformed in specific ways (i.e. feature
We present Amazon SageMaker Autopilot: a fully managed system engineering) for optimal learning. There are several other decision
that provides an automatic machine learning solution. Given a points that needs to be taken in the process; viz. picking the compute
tabular dataset and the target column name, Autopilot identifies resources to ensure the model can be trained while still keeping the
the problem type, analyzes the data and produces a diverse set of cost under control, how well the model generalizes, how efficiently,
completeML pipelines, which are tuned to generate a leaderboard of in terms of compute resources, can the model infer.
candidate models that the customer can choose from. The diversity These decisions frequently require the expertise of both a data
allows users to balance between different needs such as model scientist and a software engineer. However, there is a lack of data sci-
accuracy vs. latency. By exposing not only the final models but the ence experts, and of machine-learning informed software engineers
way they are trained, meaning the pipelines, we allow to customize in the industry, but an over-abundance of data science problems.
the generated training pipeline, thus catering the need of users with Even if the experts are available there is no escape from running
different levels of expertise. This trait is crucial for users and is the plenty of trial and error experiments to find the optimal solution
main novelty of Autopilot; it provides a solution that on one hand is for the given data.
not fully black-box and can be further worked on, while on the other With a vision to reduce these repetitive development costs, the
hand is not a do it yourself solution, requiring expertise in all aspects concept of automated machine learning (AutoML) has emerged in
of machine learning. This paper describes the different components the recent years and has become a hot area of research.
in the eco-system of Autopilot, emphasizing the infrastructure Before proceeding further, it is important to describe an ML
choices that allow scalability, high quality models, editable ML pipeline and the typical steps involved in building a good MLmodel.
pipelines, consumption of artifacts of offline meta-learning, and a Any ML based solution has 2 phases — building a good ML model
convenient integration with the entire SageMaker system allowing and using (a.k.a. deploying) that ML model. Figure 1 shows the
these trained models to be used in a production setting. typical stages of building a model.},
  doi      = {.org/10.1145/3399579.3399870},
  file     = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/das-piali-amazon-sagemaker-autopilot-a-white-box.pdf:PDF},
}

@Article{hernandeztecnicas,
  author   = {Hern{\'a}ndez, Eduardo S{\'a}nchez-Jim{\'e}nez Yasm{\'\i}n and Ortiz-Hern{\'a}ndez, Javier},
  title    = {T{\'e}cnicas de Optimizaci{\'o}n de Hiperpar{\'a}metros en Modelos de Aprendizaje Autom{\'a}tico para Predicci{\'o}n de Enfermedades Cardiovasculares},
  year     = {2017},
  file     = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/Técnicas_de_Optimización_de_Hiperparámetros_en_Modelos_de_Aprendizaje.pdf:PDF},
  keywords = {Aprendizaje automático, Enfermedades cardiovasculares, Hiperparámetros, Optimización Matemática},
}

@Book{hastie2009elements,
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H and Friedman, Jerome H},
  publisher = {Springer},
  title     = {The elements of statistical learning: data mining, inference, and prediction},
  year      = {2009},
  volume    = {2},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/hastie-trevor-the-elements-of-statistical-learning-2009.pdf:PDF},
}

@Book{geron2022hands,
  author    = {G{\'e}ron, Aur{\'e}lien},
  publisher = {" O'Reilly Media, Inc."},
  title     = {Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow},
  year      = {2022},
}

@InProceedings{Lisandra2012,
  author   = {Lisandra Bravo Ilastegui},
  title    = {PROPUESTA DE HERRAMIENTA PARA APLICAR MINERÍA DE DATOS EN ENTORNOS COMPLEJOS.},
  year     = {2012},
  abstract = {Trabajo de diploma para optar por el título de Ingeniería en Informática},
  file     = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/bravo_ilisastigui_lisandra.pdf:PDF},
}

@Misc{,
  title = {KNIME Official Site},
  url   = {www.knime.com},
}

@Book{Han2011,
  author   = {Jiawei Han, Micheline Kamber, Jian Pei},
  title    = {Data Mining. Concepts and Techniques (The Morgan Kaufmann Series in Data Management Systems)},
  year     = {2011},
  edition  = {3rd},
  abstract = {Morgan Kaufmann 2011},
  file     = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf:PDF},
}

@Book{bonaccorso2017machine,
  author    = {Bonaccorso, Giuseppe},
  publisher = {Packt Publishing Ltd},
  title     = {Machine learning algorithms},
  year      = {2017},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/machine-learning-algorithms_text-book.pdf:PDF},
}

@InProceedings{javed2022performance,
  author       = {Javed Mehedi Shamrat, FM and Ranjan, Rumesh and Hasib, Khan Md and Yadav, Amit and Siddique, Abdul Hasib},
  booktitle    = {Pervasive Computing and Social Networking: Proceedings of ICPCSN 2021},
  title        = {Performance evaluation among ID3, C4.5, and CART Decision Tree Algorithm},
  year         = {2022},
  organization = {Springer},
  pages        = {127--142},
  file         = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/PerformaceevaluationamongID3C4.5andCARTDecisionTreeAlgor.pdf:PDF},
}

@Article{gupta2017analysis,
  author    = {Gupta, Bhumika and Rawat, Aditya and Jain, Akshay and Arora, Arpit and Dhami, Naresh},
  journal   = {International Journal of Computer Applications},
  title     = {Analysis of various decision tree algorithms for classification in data mining},
  year      = {2017},
  number    = {8},
  pages     = {15--19},
  volume    = {163},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/gupta-bhumika-analysis-of-various-decision-tree.pdf:PDF},
  publisher = {Foundation of Computer Science},
}

@Article{guenther2016support,
  author    = {Guenther, Nick and Schonlau, Matthias},
  journal   = {The Stata Journal},
  title     = {Support vector machines},
  year      = {2016},
  number    = {4},
  pages     = {917--937},
  volume    = {16},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/guenther-nick-support-vector-machines-2016.pdf:PDF},
  keywords  = {st0461, svmachines, svm, statistical learning, machine learning, sup- port vector machines},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
}

@Article{abiodun2018state,
  author    = {Abiodun, Oludare Isaac and Jantan, Aman and Omolara, Abiodun Esther and Dada, Kemi Victoria and Mohamed, Nachaat AbdElatif and Arshad, Humaira},
  journal   = {Heliyon},
  title     = {State-of-the-art in artificial neural network applications: A survey},
  year      = {2018},
  number    = {11},
  pages     = {e00938},
  volume    = {4},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/abiodun-oludare-isaac-state-of-the-art-in-artificial.pdf:PDF},
  publisher = {Elsevier},
}

@Article{garcia2012survey,
  author    = {Garcia, Salvador and Luengo, Julian and S{\'a}ez, Jos{\'e} Antonio and Lopez, Victoria and Herrera, Francisco},
  journal   = {IEEE transactions on Knowledge and Data Engineering},
  title     = {A survey of discretization techniques: Taxonomy and empirical analysis in supervised learning},
  year      = {2012},
  number    = {4},
  pages     = {734--750},
  volume    = {25},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/garcia-salvador-a-survey-of-discretization-techniques.pdf:PDF},
  publisher = {IEEE},
}

@Article{Praba2021,
  author  = {R, Praba and G, Darshan and T, Roshanraj and B, Surya},
  journal = {International Journal of Scientific Research in Computer Science, Engineering and Information Technology},
  title   = {Study On Machine Learning Algorithms},
  year    = {2021},
  month   = {07},
  pages   = {67-72},
  doi     = {10.32628/CSEIT2173105},
  file    = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/r-praba-study-on-machine-learning-algorithms-2021.pdf:PDF},
}

@Book{ruppert2011statistics,
  author    = {Ruppert, David and Matteson, David S},
  publisher = {Springer},
  title     = {Statistics and data analysis for financial engineering},
  year      = {2011},
  volume    = {13},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/ruppert-david-statistics-and-data-analysis-for.pdf:PDF},
}

@Article{kotsiantis2006discretization,
  author    = {Kotsiantis, Sotiris and Kanellopoulos, Dimitris},
  journal   = {GESTS International Transactions on Computer Science and Engineering},
  title     = {Discretization techniques: A recent survey},
  year      = {2006},
  number    = {1},
  pages     = {47--58},
  volume    = {32},
  abstract  = {A discretization algorithm is needed in order to handle problems
with real-valued attributes with Decision Trees (DTs), Bayesian Networks 
(BNs) and Rule-Learners (RLs), treating the resulting intervals as nominal val-
ues. The performance of these systems is tied to the right election of these in-
tervals. A good discretization algorithm has to balance the loss of information 
intrinsic to this kind of process and generating a reasonable number of cut 
points, that is, a reasonable search space. This paper presents the well known 
discretization techniques. Of course, a single article cannot be a complete re-
view of all discretization algorithms. Despite this, we hope that the references 
cited cover the major theoretical issues and guide the researcher to interesting 
research directions and suggest possible combinations that have to be explored.},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/discretization survey paper gests.pdf:PDF},
  publisher = {Citeseer},
}

@Article{liu2002discretization,
  author    = {Liu, Huan and Hussain, Farhad and Tan, Chew Lim and Dash, Manoranjan},
  journal   = {Data mining and knowledge discovery},
  title     = {Discretization: An enabling technique},
  year      = {2002},
  pages     = {393--423},
  volume    = {6},
  abstract  = {Discrete values have important roles in data mining and knowledge discovery. They are about intervals
of numbers which are more concise to represent and specify, easier to use and comprehend as they are closer to
a knowledge-level representation than continuous values. Many studies show induction tasks can benefit from
discretization: rules with discrete values are normally shorter and more understandable and discretization can
lead to improved predictive accuracy. Furthermore, many induction algorithms found in the literature require
discrete features. All these prompt researchers and practitioners to discretize continuous features before or during
a machine learning or data mining task. There are numerous discretization methods available in the literature. It is
time for us to examine these seemingly different methods for discretization and find out how different they really
are, what are the key components of a discretization process, how we can improve the current level of research for
new development as well as the use of existing methods. This paper aims at a systematic study of discretization
methods with their history of development, effect on classification, and trade-off between speed and accuracy.
Contributions of this paper are an abstract description summarizing existing discretization methods, a hierarchical
framework to categorize the existing methods and pave the way for further development, concise discussions of
representative discretization methods, extensive experiments and their analysis, and some guidelines as to how to
choose a discretization method under various circumstances. We also identify some issues yet to solve and future
research for discretization.},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/liu1-2.pdf:PDF},
  keywords  = {discretization, continuous feature, data mining, classification},
  publisher = {Springer},
}

@Article{FI1993,
  author    = {U.M. Fayyad and K.B. Irani},
  title     = {Multi-Interval Discretization of Continuous-Valued Attributes for Classification Learning},
  year      = {1993},
  pages     = {1022--1029},
  booktitle = {13th International Joint Conference on Uncertainly in Artificial Intelligence({IJCAI93})},
  city      = {Chambery},
  country   = {France},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/fayyad1993.pdf:PDF},
}

@InProceedings{Bay2000,
  author    = {Stephen D. Bay and Department of Information and Computer Science},
  booktitle = {Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining},
  title     = {Multivariate Discretization of Continuous Variables for Set Mining},
  year      = {2000},
  pages     = {315--319},
  abstract  = {attribute-value (A-V) pairs with high predictive power, or
Many algorithms in data mining can be formulated as a set contrast set mining [4, 5] where the goal is to nd sets that
mining problem where the goal is to nd conjunctions (or represent large dierences in the probability distributions of
disjunctions) of terms that meet user specied constraints. two or more groups.
Set mining techniques have been largely designed for cat-
egorical or discrete data where variables can only take on There has been much work devoted to speeding up search},
  doi       = {doi:10.1145/347090.347159},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/347090.347159.pdf:PDF},
}

@InCollection{dougherty1995supervised,
  author    = {Dougherty, James and Kohavi, Ron and Sahami, Mehran},
  booktitle = {Machine learning proceedings 1995},
  publisher = {Elsevier},
  title     = {Supervised and unsupervised discretization of continuous features},
  year      = {1995},
  pages     = {194--202},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/disc.pdf:PDF},
}

@InProceedings{ismail2003empirical,
  author       = {Ismail, Michael and Ciesielski, Victor},
  booktitle    = {HIS},
  title        = {An Empirical Investigation of the Impact of Discretization on Common Data Distributions.},
  year         = {2003},
  organization = {Citeseer},
  pages        = {692--701},
  file         = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/document.pdf:PDF},
}

@Article{gorski2005healpix,
  author    = {Gorski, Krzysztof M and Hivon, Eric and Banday, Anthony J and Wandelt, Benjamin D and Hansen, Frode K and Reinecke, Mstvos and Bartelmann, Matthia},
  journal   = {The Astrophysical Journal},
  title     = {HEALPix: A framework for high-resolution discretization and fast analysis of data distributed on the sphere},
  year      = {2005},
  number    = {2},
  pages     = {759},
  volume    = {622},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/Górski_2005_ApJ_622_759.pdf:PDF},
  publisher = {IOP Publishing},
}

@Article{hacibeyouglu2016comparison,
  author  = {HACIBEYO{\u{G}}LU, Mehmet and IBRAHIM, Mohammed H},
  journal = {International Journal of Intelligent Systems and Applications in Engineering},
  title   = {Comparison of the effect of unsupervised and supervised discretization methods on classification process},
  year    = {2016},
  number  = {Special Issue-1},
  pages   = {105--108},
  volume  = {4},
  file    = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/10.18201-ijisae.267490-234095.pdf:PDF},
}

@InProceedings{K1992,
  author    = {R. Kerber},
  booktitle = {National Conference on Artifical Intelligence American Association for Artificial Intelligence({AAAI\'92})},
  title     = {ChiMerge: Discretization of Numeric Attributes},
  year      = {1992},
  pages     = {123--128},
  city      = {San Jos\'e},
  country   = {California USA},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/1992-Kerber-ChimErge-AAAI92.pdf:PDF},
}

@InProceedings{yang2002comparative,
  author    = {Yang, Ying and Webb, Geoffrey I},
  booktitle = {Proceedings of PKAW},
  title     = {A comparative study of discretization methods for naive-bayes classifiers},
  year      = {2002},
  volume    = {2002},
  abstract  = {Discretization is a popular approach to handling numeric
attributes in machine learning. We argue that the requirements for effec-
tive discretization differ between naive-Bayes learning and many other
learning algorithms. We evaluate the effectiveness with naive-Bayes clas-
sifiers of nine discretization methods, equal width discretization (EWD),
equal frequency discretization (EFD), fuzzy discretization (FD), entropy
minimization discretization (EMD), iterative discretization (ID), propor-
tional k-interval discretization (PKID), lazy discretization (LD), non-
disjoint discretization (NDD) and weighted proportional k-interval dis-
cretization (WPKID). It is found that in general naive-Bayes classifiers
trained on data preprocessed by LD, NDD or WPKID achieve lower
classification error than those trained on data preprocessed by the other
discretization methods. But LD can not scale to large data. This study
leads to a new discretization method, weighted non-disjoint discretiza-
tion (WNDD) that combines WPKID and NDD’s advantages. Our ex-
periments show that among all the rival discretization methods, WNDD
best helps naive-Bayes classifiers reduce average classification error.},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/YangWebb02a.pdf:PDF},
}

@InProceedings{gama1998dynamic,
  author       = {Gama, Joao and Torgo, Luis and Soares, Carlos},
  booktitle    = {Progress in Artificial Intelligence—IBERAMIA 98: 6th Ibero-American Conference on AI Lisbon, Portugal, October 5--9, 1998 Proceedings 6},
  title        = {Dynamic discretization of continuous attributes},
  year         = {1998},
  organization = {Springer},
  pages        = {160--169},
  abstract     = {Discretization of continuous attributes is an important task
for certain types of machine learning algorithms. Bayesian approaches,
for instance, require assumptions about data distributions. Decision
Trees, on the other hand, require sorting operations to deal with con-
tinuous attributes, which largely increase learning times. This paper
presents a new method of discretization, whose main characteristic is
that it takes into account interdependencies between attributes. Detect-
ing interdependencies can be seen as discovering redundant attributes.
This means that our method performs attribute selection as a side effect
of the discretization. Empirical evaluation on five benchmark datasets
from UCI repository, using C4.5 and a naive Bayes, shows a consistent
reduction of the features without loss of generalization accuracy.},
  file         = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/gama-joão-dynamic-discretization-of-continuous.pdf:PDF},
  keywords     = {Discretization, Feature Selection, Continuous Attributes},
  ranking      = {rank4},
  relevance    = {relevant},
}

@Article{mishra2019descriptive,
  author    = {Mishra, Prabhaker and Pandey, Chandra M and Singh, Uttam and Gupta, Anshul and Sahu, Chinmoy and Keshri, Amit},
  journal   = {Annals of cardiac anaesthesia},
  title     = {Descriptive statistics and normality tests for statistical data},
  year      = {2019},
  number    = {1},
  pages     = {67},
  volume    = {22},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/ACA-22-67.pdf:PDF},
  keywords  = {Biomedical research, descriptive statistics, numerical and visual methods, test of Gandhi Postgraduate Institute normality of Medical Sciences, Lucknow},
  publisher = {Wolters Kluwer--Medknow Publications},
}

@Book{sheskin2020handbook,
  author    = {Sheskin, David J},
  publisher = {crc Press},
  title     = {Handbook of parametric and nonparametric statistical procedures},
  year      = {2020},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/sheskin-handbook-of-parametric-and-nonparametric.pdf:PDF},
  journal   = {The American Statistician},
  pages     = {374},
}

@InProceedings{ventura1995empirical,
  author    = {Ventura, Dan and Martinez, Tony R},
  booktitle = {Proceedings of the Tenth International Symposium on Computer and Information Sciences},
  title     = {An empirical comparison of discretization methods},
  year      = {1995},
  pages     = {443--450},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/ventura1995iscis.pdf:PDF},
}

@Article{kareem2014improved,
  author  = {Kareem, Ihsan A and Duaimi, Mehdi G},
  journal = {International Journal of Computer Science and Mobile Computing},
  title   = {Improved accuracy for decision tree algorithm based on unsupervised discretization},
  year    = {2014},
  number  = {6},
  pages   = {176--183},
  volume  = {3},
  file    = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/fb76a606bb77162ebdf737a25d84d65e.pdf:PDF},
}

@InProceedings{torgo1997search,
  author       = {Torgo, Lu{\'\i}s and Gama, Jo{\~a}o},
  booktitle    = {Machine Learning: ECML-97: 9th European Conference on Machine Learning Prague, Czech Republic, April 23--25, 1997 Proceedings 9},
  title        = {Search-based class discretization},
  year         = {1997},
  organization = {Springer},
  pages        = {266--273},
  abstract     = {We present a methodology that enables the use of classification algorithms on
regression tasks. We implement this method in system RECLA that transforms a regression 
problem into a classification one and then uses an existent classification system to solve this 
new problem. The transformation consists of mapping a continuous variable into an ordinal 
variable by grouping its values into an appropriate set of intervals. We use misclassification 
costs as a means to reflect the implicit ordering among the ordinal values of the new 
variable. We describe a set of alternative discretization methods and, based on our 
experimental results, justify the need for a search-based approach to choose the best method. 
Our experimental results confirm the validity of our search-based approach to class 
discretization, and reveal the accuracy benefits of adding misclassification costs.},
  file         = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/torgo-luís-search-based-class-discretization-1997.pdf:PDF},
  keywords     = {: Regression, Classification, Discretization methods},
}

@Article{kurgan2004caim,
  author    = {Kurgan, Lukasz A and Cios, Krzysztof J},
  journal   = {IEEE transactions on Knowledge and Data Engineering},
  title     = {CAIM discretization algorithm},
  year      = {2004},
  number    = {2},
  pages     = {145--153},
  volume    = {16},
  abstract  = {The task of extracting knowledge from databases is quite often performed by machine learning algorithms. The majority of},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/2004-Kurgan-IEEETKDE.pdf:PDF},
  publisher = {IEEE},
}

@Article{yang2009discretization,
  author    = {Yang, Ying and Webb, Geoffrey I},
  journal   = {Machine learning},
  title     = {Discretization for naive-Bayes learning: managing discretization bias and variance},
  year      = {2009},
  pages     = {39--74},
  volume    = {74},
  abstract  = {Quantitative attributes are usually discretized in Naive-Bayes learning. We estab-
lish simple conditions under which discretization is equivalent to use of the true probability
density function during naive-Bayes learning. The use of different discretization techniques
can be expected to affect the classification bias and variance of generated naive-Bayes classi-
fiers, effects we name discretization bias and variance. We argue that by properly managing
discretization bias and variance, we can effectively reduce naive-Bayes classification error.
In particular, we supply insights into managing discretization bias and variance by adjusting
the number of intervals and the number of training instances contained in each interval. We
accordingly propose proportional discretization and fixed frequency discretization, two effi-
cient unsupervised discretization methods that are able to effectively manage discretization
bias and variance. We evaluate our new techniques against four key discretization meth-
ods for naive-Bayes classifiers. The experimental results support our theoretical analyses
by showing that with statistically significant frequency, naive-Bayes classifiers trained on
data discretized by our new methods are able to achieve lower classification error than those
trained on data discretized by current established discretization methods.},
  doi       = {10.1007/s10994-008-5083-5},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/2009-Yang-ML.pdf:PDF},
  keywords  = {Discretization · Naive-Bayes Learning · Bias · Variance},
  publisher = {Springer},
}

@Comment{jabref-meta: databaseType:bibtex;}
