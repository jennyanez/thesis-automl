@Article{he2021automl,
  author    = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  journal   = {Knowledge-Based Systems},
  title     = {AutoML: A survey of the state-of-the-art},
  year      = {2021},
  pages     = {106622},
  volume    = {212},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/he-xin-automl-a-survey-of-the-state-of-the-art.pdf:PDF},
  publisher = {Elsevier},
}

@Book{hutter2019automated,
  author    = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  publisher = {Springer Nature},
  title     = {Automated machine learning: methods, systems, challenges},
  year      = {2019},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/Frank Hutter Automated Machine Learning.pdf:PDF},
}

@Article{zoller2021benchmark,
  author  = {Z{\"o}ller, Marc-Andr{\'e} and Huber, Marco F},
  journal = {Journal of artificial intelligence research},
  title   = {Benchmark and survey of automated machine learning frameworks},
  year    = {2021},
  pages   = {409--472},
  volume  = {70},
  file    = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/zöller-marc-andré-benchmark-and-survey-of-automated.pdf:PDF},
}

@InProceedings{tuggener2019automated,
  author       = {Tuggener, Lukas and Amirian, Mohammadreza and Rombach, Katharina and L{\"o}rwald, Stefan and Varlet, Anastasia and Westermann, Christian and Stadelmann, Thilo},
  booktitle    = {2019 6th Swiss Conference on Data Science (SDS)},
  title        = {Automated machine learning in practice: state of the art and recent results},
  year         = {2019},
  organization = {IEEE},
  pages        = {31--36},
  file         = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/tuggener2019.pdf:PDF},
}

@Article{moreno2013comparative,
  author    = {Moreno, Alberto Prieto and Santiago, Orestes Llanes and de Lazaro, Jose Manuel Bernal and Moreno, Emilio Garcia},
  journal   = {IEEE Latin America Transactions},
  title     = {Comparative evaluation of classification methods used in fault diagnosis of industrial processes},
  year      = {2013},
  number    = {2},
  pages     = {682--689},
  volume    = {11},
  abstract  = {This article presents a comparative study of the obtención de los modelos adecuados, se usan otros enfoques
performance of classification techniques used for fault diagnosis como son la aplicación de técnicas de inteligencia artificial 
in industrial processes. The techniques studied ranging from para capturar el conocimiento experto y la utilización de 
classifiers based on Bayes theory as Maximum a Posteriori 
Probability (MAP) and Nearest Neighbor (kNN) classifiers, técnicas estadísticas multivariadas y de reconocimiento de},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/prietomoreno2013.pdf:PDF},
  keywords  = {industrial processes, fault diagnosis, support vector que las cinco herramientas de clasificación más difundidas machines, artificial neural networks, partial least squares, son: el clasificador de Máxima Probabilidad a Posteriori nearest neighbors classifier, MAP classifier},
  publisher = {IEEE},
}

@Book{sammut2011encyclopedia,
  author    = {Sammut, Claude and Webb, Geoffrey I},
  publisher = {Springer Science \& Business Media},
  title     = {Encyclopedia of machine learning},
  year      = {2011},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/sammut-claude-encyclopedia-of-machine-learning-2010.pdf:PDF},
}

@InProceedings{Feurer2019,
  author   = {Matthias Feurer and Aaron Klein and Katharina Eggensperger and Jost Tobias Springenberg and Manuel Blum and Frank Hutter},
  title    = {Chapter 6 Auto-sklearn: Efficient and Robust Automated Machine Learning},
  year     = {2019},
  abstract = {The success of machine learning in a broad range of applications has led
to an ever-growing demand for machine learning systems that can be used off the
shelf by non-experts. To be effective in practice, such systems need to automatically
choose a good algorithm and feature preprocessing steps for a new dataset at hand,
and also set their respective hyperparameters. Recent work has started to tackle this
automated machine learning (AutoML) problem with the help of efficient Bayesian
optimization methods. Building on this, we introduce a robust new AutoML system
based on the Python machine learning package scikit-learn (using 15 classifiers, 14
feature preprocessing methods, and 4 data preprocessing methods, giving rise to a
structured hypothesis space with 110 hyperparameters). This system, which we dub
Auto-sklearn, improves on existing AutoML methods by automatically taking into
account past performance on similar datasets, and by constructing ensembles from
the models evaluated during the optimization. Our system won six out of ten phases
of the first ChaLearn AutoML challenge, and our comprehensive analysis on over
100 diverse datasets shows that it substantially outperforms the previous state of
the art in AutoML. We also demonstrate the performance gains due to each of our
contributions and derive insights into the effectiveness of the individual components
of Auto-sklearn.},
  doi      = {.org/10.1007/978-3-030-05318-5_6},
  file     = {:Feurer2019 - Chapter 6 Auto Sklearn_ Efficient and Robust Automated Machine Learning.pdf:PDF;:liu1-2.pdf:PDF},
}

@Article{waring2020automated,
  author    = {Waring, Jonathan and Lindvall, Charlotta and Umeton, Renato},
  journal   = {Artificial intelligence in medicine},
  title     = {Automated machine learning: Review of the state-of-the-art and opportunities for healthcare},
  year      = {2020},
  pages     = {101822},
  volume    = {104},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/waring-jonathan-automated-machine-learning-review-of.pdf:PDF},
  publisher = {Elsevier},
}

@Book{orallo2004,
  author    = {Hern{\'a}ndez Orallo, Jos{\'e} and others},
  publisher = {Biblioteca Hern{\'a}n Malo Gonz{\'a}lez},
  title     = {Introducci{\'o}n a la Miner{\'\i}a de Datos},
  year      = {2004},
  file      = {:D\:/School/Informatica/3er Año/1er trimestre/Practicas/Data mining/curso mineria de datos y knime/bibliografía/Libro Introducción a la MD (Hernández Orallo).pdf:PDF},
}

@InProceedings{Agrawal1519,
  author   = {Rakesh Agrawal and Tomasz Imielinski and Arun Swami},
  title    = {Mining Association Rules between Sets of Items in Large Databases},
  year     = {1993},
  abstract = {on tertiary storage and are very slowly migrating to},
  file     = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/agrawal1993.pdf:PDF},
}

@InProceedings{moine2011estudio,
  author    = {Moine, Juan Miguel and Haedo, Ana Silvia and Gordillo, Silvia Ethel},
  booktitle = {XIII Workshop de Investigadores en Ciencias de la Computaci{\'o}n},
  title     = {Estudio comparativo de metodolog{\'\i}as para miner{\'\i}a de datos},
  year      = {2011},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/Estudio comparativo de metodologías para minería de datos..pdf:PDF},
}

@InProceedings{Han1999,
  author = {Han, Jiawei.; Kamber, Micheline.},
  title  = {Data Mining : Concepts and Techniques},
  year   = {1999},
  file   = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/data-mining-concepts-and-techniques-2nd-edition-impressao.pdf:PDF},
}

@Book{murphy2012machine,
  author    = {Murphy, Kevin P},
  publisher = {MIT press},
  title     = {Machine learning: a probabilistic perspective},
  year      = {2012},
  file      = {:murphy2012machine - Machine Learning_ a Probabilistic Perspective.pdf:PDF;:Data Preprocessing in Data Mining.pdf:PDF},
}

@Book{garcia2015data,
  author    = {Garc{\'\i}a, Salvador and Luengo, Juli{\'a}n and Herrera, Francisco},
  publisher = {Springer},
  title     = {Data preprocessing in data mining},
  year      = {2015},
  file      = {:garcia2015data - Data Preprocessing in Data Mining.pdf:PDF;:ACA-22-67.pdf:PDF;:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/garcia2015data - Data Preprocessing in Data Mining.pdf:PDF},
}

@InProceedings{Carrazana2022,
  author = {Carrazana Ruiz, Ernesto},
  title  = {Componente KNIME de AutoML para pre-procesado en tareas de Clasificación},
  year   = {2022},
  file   = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/cosas de ernesto/Tesis_Ernesto_Carrazana_Ruiz_IC/Tesis_Ernesto_Carrazana_Ruiz_IC/Tesis_Ernesto_Carrazana_Ruiz_IC.pdf:PDF},
}

@Article{celik2022online,
  author    = {Celik, Bilge and Singh, Prabhant and Vanschoren, Joaquin},
  journal   = {Machine Learning},
  title     = {Online AutoML: An adaptive AutoML framework for online learning},
  year      = {2022},
  pages     = {1--25},
  abstract  = {Automated Machine Learning (AutoML) has been used successfully in settings where 
the learning task is assumed to be static. In many real-world scenarios, however, the data 
distribution will evolve over time, and it is yet to be shown whether AutoML techniques 
can effectively design online pipelines in dynamic environments. This study aims to auto-
mate pipeline design for online learning while continuously adapting to data drift. For this 
purpose, we design an adaptive Online Automated Machine Learning (OAML) system, 
searching the complete pipeline configuration space of online learners, including preproc-
essing algorithms and ensembling techniques. This system combines the inherent adapta-
tion capabilities of online learners with fast automated pipeline (re)optimization. Focusing 
on optimization techniques that can adapt to evolving objectives, we evaluate asynchronous 
genetic programming and asynchronous successive halving to optimize these pipelines 
continually. We experiment on real and artificial data streams with varying types of con-
cept drift to test the performance and adaptation capabilities of the proposed system. The 
results confirm the utility of OAML over popular online learning algorithms and under-
score the benefits of continuous pipeline redesign in the presence of data drift.},
  doi       = {.org/10.1007/s10994-022-06262-0},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/celik-bilge-online-automl-an-adaptive-automl-framework.pdf:PDF},
  keywords  = {Online automl · Automated online learning · Concept drift · Automated drift adaptation},
  publisher = {Springer},
}

@Article{soh2020introduction,
  author    = {Soh, Julian and Singh, Priyanshi and Soh, Julian and Singh, Priyanshi},
  journal   = {Data Science Solutions on Azure: Tools and Techniques Using Databricks and MLOps},
  title     = {Introduction to azure machine learning},
  year      = {2020},
  pages     = {117--148},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/soh-julian-introduction-to-azure-machine-learning-2020.pdf:PDF},
  publisher = {Springer},
}

@InProceedings{Das2020,
  author   = {Piali Das and Nikita Ivkin and Tanya Bansal and Laurence Rouesnel and Philip Gautier and Zohar Karnin and Leo Dirac and Lakshmi Ramakrishnan and Andre Perunicic and Iaroslav Shcherbatyi and Wilton Wu},
  title    = {Amazon SageMaker Autopilot: a white box AutoML solution at scale},
  year     = {2020},
  abstract = {data to be prepared and transformed in specific ways (i.e. feature
We present Amazon SageMaker Autopilot: a fully managed system engineering) for optimal learning. There are several other decision
that provides an automatic machine learning solution. Given a points that needs to be taken in the process; viz. picking the compute
tabular dataset and the target column name, Autopilot identifies resources to ensure the model can be trained while still keeping the
the problem type, analyzes the data and produces a diverse set of cost under control, how well the model generalizes, how efficiently,
completeML pipelines, which are tuned to generate a leaderboard of in terms of compute resources, can the model infer.
candidate models that the customer can choose from. The diversity These decisions frequently require the expertise of both a data
allows users to balance between different needs such as model scientist and a software engineer. However, there is a lack of data sci-
accuracy vs. latency. By exposing not only the final models but the ence experts, and of machine-learning informed software engineers
way they are trained, meaning the pipelines, we allow to customize in the industry, but an over-abundance of data science problems.
the generated training pipeline, thus catering the need of users with Even if the experts are available there is no escape from running
different levels of expertise. This trait is crucial for users and is the plenty of trial and error experiments to find the optimal solution
main novelty of Autopilot; it provides a solution that on one hand is for the given data.
not fully black-box and can be further worked on, while on the other With a vision to reduce these repetitive development costs, the
hand is not a do it yourself solution, requiring expertise in all aspects concept of automated machine learning (AutoML) has emerged in
of machine learning. This paper describes the different components the recent years and has become a hot area of research.
in the eco-system of Autopilot, emphasizing the infrastructure Before proceeding further, it is important to describe an ML
choices that allow scalability, high quality models, editable ML pipeline and the typical steps involved in building a good MLmodel.
pipelines, consumption of artifacts of offline meta-learning, and a Any ML based solution has 2 phases — building a good ML model
convenient integration with the entire SageMaker system allowing and using (a.k.a. deploying) that ML model. Figure 1 shows the
these trained models to be used in a production setting. typical stages of building a model.},
  doi      = {.org/10.1145/3399579.3399870},
  file     = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/das-piali-amazon-sagemaker-autopilot-a-white-box.pdf:PDF},
}

@Article{hernandeztecnicas,
  author   = {Hern{\'a}ndez, Eduardo S{\'a}nchez-Jim{\'e}nez Yasm{\'\i}n and Ortiz-Hern{\'a}ndez, Javier},
  title    = {T{\'e}cnicas de Optimizaci{\'o}n de Hiperpar{\'a}metros en Modelos de Aprendizaje Autom{\'a}tico para Predicci{\'o}n de Enfermedades Cardiovasculares},
  year     = {2017},
  file     = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/Técnicas_de_Optimización_de_Hiperparámetros_en_Modelos_de_Aprendizaje.pdf:PDF},
  keywords = {Aprendizaje automático, Enfermedades cardiovasculares, Hiperparámetros, Optimización Matemática},
}

@Book{hastie2009elements,
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H and Friedman, Jerome H},
  publisher = {Springer},
  title     = {The elements of statistical learning: data mining, inference, and prediction},
  year      = {2009},
  volume    = {2},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/hastie-trevor-the-elements-of-statistical-learning-2009.pdf:PDF},
}

@Book{geron2022hands,
  author    = {G{\'e}ron, Aur{\'e}lien},
  publisher = {" O'Reilly Media, Inc."},
  title     = {Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow},
  year      = {2022},
}

@InProceedings{Lisandra2012,
  author   = {Bravo Ilisástigui, Lisandra},
  title    = {PROPUESTA DE HERRAMIENTA PARA APLICAR MINERÍA DE DATOS EN ENTORNOS COMPLEJOS.},
  year     = {2012},
  abstract = {Trabajo de diploma para optar por el título de Ingeniería en Informática},
  file     = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/bravo_ilisastigui_lisandra.pdf:PDF},
}

@Misc{knime2023,
  author    = {KNIME},
  month     = mar,
  note      = {Visitado el 10 de marzo de 2023},
  title     = {KNIME Official Site},
  year      = {2023},
  timestamp = {2023-03-10},
  url       = {www.knime.com},
}

@Book{Han2011,
  author   = {Jiawei Han, Micheline Kamber, Jian Pei},
  title    = {Data Mining. Concepts and Techniques (The Morgan Kaufmann Series in Data Management Systems)},
  year     = {2011},
  edition  = {3rd},
  abstract = {Morgan Kaufmann 2011},
  file     = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf:PDF},
}

@Book{bonaccorso2017machine,
  author    = {Bonaccorso, Giuseppe},
  publisher = {Packt Publishing Ltd},
  title     = {Machine learning algorithms},
  year      = {2017},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/machine-learning-algorithms_text-book.pdf:PDF},
}

@InProceedings{javed2022performance,
  author       = {Javed Mehedi Shamrat, FM and Ranjan, Rumesh and Hasib, Khan Md and Yadav, Amit and Siddique, Abdul Hasib},
  booktitle    = {Pervasive Computing and Social Networking: Proceedings of ICPCSN 2021},
  title        = {Performance evaluation among ID3, C4.5, and CART Decision Tree Algorithm},
  year         = {2022},
  organization = {Springer},
  pages        = {127--142},
  file         = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/PerformaceevaluationamongID3C4.5andCARTDecisionTreeAlgor.pdf:PDF},
}

@Article{gupta2017analysis,
  author    = {Gupta, Bhumika and Rawat, Aditya and Jain, Akshay and Arora, Arpit and Dhami, Naresh},
  journal   = {International Journal of Computer Applications},
  title     = {Analysis of various decision tree algorithms for classification in data mining},
  year      = {2017},
  number    = {8},
  pages     = {15--19},
  volume    = {163},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/gupta-bhumika-analysis-of-various-decision-tree.pdf:PDF},
  publisher = {Foundation of Computer Science},
}

@Article{guenther2016support,
  author    = {Guenther, Nick and Schonlau, Matthias},
  journal   = {The Stata Journal},
  title     = {Support vector machines},
  year      = {2016},
  number    = {4},
  pages     = {917--937},
  volume    = {16},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/guenther-nick-support-vector-machines-2016.pdf:PDF},
  keywords  = {st0461, svmachines, svm, statistical learning, machine learning, sup- port vector machines},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
}

@Article{abiodun2018state,
  author    = {Abiodun, Oludare Isaac and Jantan, Aman and Omolara, Abiodun Esther and Dada, Kemi Victoria and Mohamed, Nachaat AbdElatif and Arshad, Humaira},
  journal   = {Heliyon},
  title     = {State-of-the-art in artificial neural network applications: A survey},
  year      = {2018},
  number    = {11},
  pages     = {e00938},
  volume    = {4},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/abiodun-oludare-isaac-state-of-the-art-in-artificial.pdf:PDF},
  publisher = {Elsevier},
}

@Article{garcia2012survey,
  author    = {Garcia, Salvador and Luengo, Julian and S{\'a}ez, Jos{\'e} Antonio and Lopez, Victoria and Herrera, Francisco},
  journal   = {IEEE transactions on Knowledge and Data Engineering},
  title     = {A survey of discretization techniques: Taxonomy and empirical analysis in supervised learning},
  year      = {2012},
  number    = {4},
  pages     = {734--750},
  volume    = {25},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/garcia-salvador-a-survey-of-discretization-techniques.pdf:PDF},
  publisher = {IEEE},
}

@Article{Praba2021,
  author  = {Praba, R and Darshan, G and Roshanraj, T and Surya, B},
  journal = {International Journal of Scientific Research in Computer Science, Engineering and Information Technology},
  title   = {Study On Machine Learning Algorithms},
  year    = {2021},
  month   = {07},
  pages   = {67-72},
  doi     = {10.32628/CSEIT2173105},
  file    = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/r-praba-study-on-machine-learning-algorithms-2021.pdf:PDF},
}

@Book{ruppert2011statistics,
  author    = {Ruppert, David and Matteson, David S},
  publisher = {Springer},
  title     = {Statistics and data analysis for financial engineering},
  year      = {2011},
  volume    = {13},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/ruppert-david-statistics-and-data-analysis-for.pdf:PDF},
}

@Article{kotsiantis2006discretization,
  author    = {Kotsiantis, Sotiris and Kanellopoulos, Dimitris},
  journal   = {GESTS International Transactions on Computer Science and Engineering},
  title     = {Discretization techniques: A recent survey},
  year      = {2006},
  number    = {1},
  pages     = {47--58},
  volume    = {32},
  abstract  = {A discretization algorithm is needed in order to handle problems
with real-valued attributes with Decision Trees (DTs), Bayesian Networks 
(BNs) and Rule-Learners (RLs), treating the resulting intervals as nominal val-
ues. The performance of these systems is tied to the right election of these in-
tervals. A good discretization algorithm has to balance the loss of information 
intrinsic to this kind of process and generating a reasonable number of cut 
points, that is, a reasonable search space. This paper presents the well known 
discretization techniques. Of course, a single article cannot be a complete re-
view of all discretization algorithms. Despite this, we hope that the references 
cited cover the major theoretical issues and guide the researcher to interesting 
research directions and suggest possible combinations that have to be explored.},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/discretization survey paper gests.pdf:PDF},
  publisher = {Citeseer},
}

@Article{liu2002discretization,
  author    = {Liu, Huan and Hussain, Farhad and Tan, Chew Lim and Dash, Manoranjan},
  journal   = {Data mining and knowledge discovery},
  title     = {Discretization: An enabling technique},
  year      = {2002},
  pages     = {393--423},
  volume    = {6},
  abstract  = {Discrete values have important roles in data mining and knowledge discovery. They are about intervals
of numbers which are more concise to represent and specify, easier to use and comprehend as they are closer to
a knowledge-level representation than continuous values. Many studies show induction tasks can benefit from
discretization: rules with discrete values are normally shorter and more understandable and discretization can
lead to improved predictive accuracy. Furthermore, many induction algorithms found in the literature require
discrete features. All these prompt researchers and practitioners to discretize continuous features before or during
a machine learning or data mining task. There are numerous discretization methods available in the literature. It is
time for us to examine these seemingly different methods for discretization and find out how different they really
are, what are the key components of a discretization process, how we can improve the current level of research for
new development as well as the use of existing methods. This paper aims at a systematic study of discretization
methods with their history of development, effect on classification, and trade-off between speed and accuracy.
Contributions of this paper are an abstract description summarizing existing discretization methods, a hierarchical
framework to categorize the existing methods and pave the way for further development, concise discussions of
representative discretization methods, extensive experiments and their analysis, and some guidelines as to how to
choose a discretization method under various circumstances. We also identify some issues yet to solve and future
research for discretization.},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/liu1-2.pdf:PDF},
  keywords  = {discretization, continuous feature, data mining, classification},
  publisher = {Springer},
}

@Article{FI1993,
  author    = {U.M. Fayyad and K.B. Irani},
  title     = {Multi-Interval Discretization of Continuous-Valued Attributes for Classification Learning},
  year      = {1993},
  pages     = {1022--1029},
  booktitle = {13th International Joint Conference on Uncertainly in Artificial Intelligence({IJCAI93})},
  city      = {Chambery},
  country   = {France},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/fayyad1993.pdf:PDF},
}

@InProceedings{Bay2000,
  author    = {Stephen D. Bay and Department of Information and Computer Science},
  booktitle = {Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining},
  title     = {Multivariate Discretization of Continuous Variables for Set Mining},
  year      = {2000},
  pages     = {315--319},
  abstract  = {attribute-value (A-V) pairs with high predictive power, or
Many algorithms in data mining can be formulated as a set contrast set mining [4, 5] where the goal is to nd sets that
mining problem where the goal is to nd conjunctions (or represent large dierences in the probability distributions of
disjunctions) of terms that meet user specied constraints. two or more groups.
Set mining techniques have been largely designed for cat-
egorical or discrete data where variables can only take on There has been much work devoted to speeding up search},
  doi       = {doi:10.1145/347090.347159},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/347090.347159.pdf:PDF},
}

@InCollection{dougherty1995supervised,
  author    = {Dougherty, James and Kohavi, Ron and Sahami, Mehran},
  booktitle = {Machine learning proceedings 1995},
  publisher = {Elsevier},
  title     = {Supervised and unsupervised discretization of continuous features},
  year      = {1995},
  pages     = {194--202},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/disc.pdf:PDF},
}

@InProceedings{ismail2003empirical,
  author       = {Ismail, Michael and Ciesielski, Victor},
  booktitle    = {HIS},
  title        = {An Empirical Investigation of the Impact of Discretization on Common Data Distributions.},
  year         = {2003},
  organization = {Citeseer},
  pages        = {692--701},
  file         = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/document.pdf:PDF},
}

@Article{gorski2005healpix,
  author    = {Gorski, Krzysztof M and Hivon, Eric and Banday, Anthony J and Wandelt, Benjamin D and Hansen, Frode K and Reinecke, Mstvos and Bartelmann, Matthia},
  journal   = {The Astrophysical Journal},
  title     = {HEALPix: A framework for high-resolution discretization and fast analysis of data distributed on the sphere},
  year      = {2005},
  number    = {2},
  pages     = {759},
  volume    = {622},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/Górski_2005_ApJ_622_759.pdf:PDF},
  publisher = {IOP Publishing},
}

@Article{hacibeyouglu2016comparison,
  author  = {HACIBEYO{\u{G}}LU, Mehmet and IBRAHIM, Mohammed H},
  journal = {International Journal of Intelligent Systems and Applications in Engineering},
  title   = {Comparison of the effect of unsupervised and supervised discretization methods on classification process},
  year    = {2016},
  number  = {Special Issue-1},
  pages   = {105--108},
  volume  = {4},
  file    = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/10.18201-ijisae.267490-234095.pdf:PDF},
}

@InProceedings{K1992,
  author    = {R. Kerber},
  booktitle = {National Conference on Artifical Intelligence American Association for Artificial Intelligence({AAAI\'92})},
  title     = {ChiMerge: Discretization of Numeric Attributes},
  year      = {1992},
  pages     = {123--128},
  city      = {San Jos\'e},
  country   = {California USA},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/1992-Kerber-ChimErge-AAAI92.pdf:PDF},
}

@InProceedings{yang2002comparative,
  author    = {Yang, Ying and Webb, Geoffrey I},
  booktitle = {Proceedings of PKAW},
  title     = {A comparative study of discretization methods for naive-bayes classifiers},
  year      = {2002},
  volume    = {2002},
  abstract  = {Discretization is a popular approach to handling numeric
attributes in machine learning. We argue that the requirements for effec-
tive discretization differ between naive-Bayes learning and many other
learning algorithms. We evaluate the effectiveness with naive-Bayes clas-
sifiers of nine discretization methods, equal width discretization (EWD),
equal frequency discretization (EFD), fuzzy discretization (FD), entropy
minimization discretization (EMD), iterative discretization (ID), propor-
tional k-interval discretization (PKID), lazy discretization (LD), non-
disjoint discretization (NDD) and weighted proportional k-interval dis-
cretization (WPKID). It is found that in general naive-Bayes classifiers
trained on data preprocessed by LD, NDD or WPKID achieve lower
classification error than those trained on data preprocessed by the other
discretization methods. But LD can not scale to large data. This study
leads to a new discretization method, weighted non-disjoint discretiza-
tion (WNDD) that combines WPKID and NDD’s advantages. Our ex-
periments show that among all the rival discretization methods, WNDD
best helps naive-Bayes classifiers reduce average classification error.},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/YangWebb02a.pdf:PDF},
}

@InProceedings{gama1998dynamic,
  author       = {Gama, Joao and Torgo, Luis and Soares, Carlos},
  booktitle    = {Progress in Artificial Intelligence—IBERAMIA 98: 6th Ibero-American Conference on AI Lisbon, Portugal, October 5--9, 1998 Proceedings 6},
  title        = {Dynamic discretization of continuous attributes},
  year         = {1998},
  organization = {Springer},
  pages        = {160--169},
  abstract     = {Discretization of continuous attributes is an important task
for certain types of machine learning algorithms. Bayesian approaches,
for instance, require assumptions about data distributions. Decision
Trees, on the other hand, require sorting operations to deal with con-
tinuous attributes, which largely increase learning times. This paper
presents a new method of discretization, whose main characteristic is
that it takes into account interdependencies between attributes. Detect-
ing interdependencies can be seen as discovering redundant attributes.
This means that our method performs attribute selection as a side effect
of the discretization. Empirical evaluation on five benchmark datasets
from UCI repository, using C4.5 and a naive Bayes, shows a consistent
reduction of the features without loss of generalization accuracy.},
  file         = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/gama-joão-dynamic-discretization-of-continuous.pdf:PDF},
  keywords     = {Discretization, Feature Selection, Continuous Attributes},
  ranking      = {rank4},
  relevance    = {relevant},
}

@Article{mishra2019descriptive,
  author    = {Mishra, Prabhaker and Pandey, Chandra M and Singh, Uttam and Gupta, Anshul and Sahu, Chinmoy and Keshri, Amit},
  journal   = {Annals of cardiac anaesthesia},
  title     = {Descriptive statistics and normality tests for statistical data},
  year      = {2019},
  number    = {1},
  pages     = {67},
  volume    = {22},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/ACA-22-67.pdf:PDF},
  keywords  = {Biomedical research, descriptive statistics, numerical and visual methods, test of Gandhi Postgraduate Institute normality of Medical Sciences, Lucknow},
  publisher = {Wolters Kluwer--Medknow Publications},
}

@Book{sheskin2020handbook,
  author    = {Sheskin, David J},
  publisher = {crc Press},
  title     = {Handbook of parametric and nonparametric statistical procedures},
  year      = {2020},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/sheskin-handbook-of-parametric-and-nonparametric.pdf:PDF},
  journal   = {The American Statistician},
  pages     = {374},
}

@InProceedings{ventura1995empirical,
  author    = {Ventura, Dan and Martinez, Tony R},
  booktitle = {Proceedings of the Tenth International Symposium on Computer and Information Sciences},
  title     = {An empirical comparison of discretization methods},
  year      = {1995},
  pages     = {443--450},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/ventura1995iscis.pdf:PDF},
}

@Article{kareem2014improved,
  author  = {Kareem, Ihsan A and Duaimi, Mehdi G},
  journal = {International Journal of Computer Science and Mobile Computing},
  title   = {Improved accuracy for decision tree algorithm based on unsupervised discretization},
  year    = {2014},
  number  = {6},
  pages   = {176--183},
  volume  = {3},
  file    = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/fb76a606bb77162ebdf737a25d84d65e.pdf:PDF},
}

@InProceedings{torgo1997search,
  author       = {Torgo, Lu{\'\i}s and Gama, Jo{\~a}o},
  booktitle    = {Machine Learning: ECML-97: 9th European Conference on Machine Learning Prague, Czech Republic, April 23--25, 1997 Proceedings 9},
  title        = {Search-based class discretization},
  year         = {1997},
  organization = {Springer},
  pages        = {266--273},
  abstract     = {We present a methodology that enables the use of classification algorithms on
regression tasks. We implement this method in system RECLA that transforms a regression 
problem into a classification one and then uses an existent classification system to solve this 
new problem. The transformation consists of mapping a continuous variable into an ordinal 
variable by grouping its values into an appropriate set of intervals. We use misclassification 
costs as a means to reflect the implicit ordering among the ordinal values of the new 
variable. We describe a set of alternative discretization methods and, based on our 
experimental results, justify the need for a search-based approach to choose the best method. 
Our experimental results confirm the validity of our search-based approach to class 
discretization, and reveal the accuracy benefits of adding misclassification costs.},
  file         = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/torgo-luís-search-based-class-discretization-1997.pdf:PDF},
  keywords     = {: Regression, Classification, Discretization methods},
}

@Article{kurgan2004caim,
  author    = {Kurgan, Lukasz A and Cios, Krzysztof J},
  journal   = {IEEE transactions on Knowledge and Data Engineering},
  title     = {CAIM discretization algorithm},
  year      = {2004},
  number    = {2},
  pages     = {145--153},
  volume    = {16},
  abstract  = {The task of extracting knowledge from databases is quite often performed by machine learning algorithms. The majority of},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/2004-Kurgan-IEEETKDE.pdf:PDF},
  publisher = {IEEE},
}

@Article{yang2009discretization,
  author    = {Yang, Ying and Webb, Geoffrey I},
  journal   = {Machine learning},
  title     = {Discretization for naive-Bayes learning: managing discretization bias and variance},
  year      = {2009},
  pages     = {39--74},
  volume    = {74},
  abstract  = {Quantitative attributes are usually discretized in Naive-Bayes learning. We estab-
lish simple conditions under which discretization is equivalent to use of the true probability
density function during naive-Bayes learning. The use of different discretization techniques
can be expected to affect the classification bias and variance of generated naive-Bayes classi-
fiers, effects we name discretization bias and variance. We argue that by properly managing
discretization bias and variance, we can effectively reduce naive-Bayes classification error.
In particular, we supply insights into managing discretization bias and variance by adjusting
the number of intervals and the number of training instances contained in each interval. We
accordingly propose proportional discretization and fixed frequency discretization, two effi-
cient unsupervised discretization methods that are able to effectively manage discretization
bias and variance. We evaluate our new techniques against four key discretization meth-
ods for naive-Bayes classifiers. The experimental results support our theoretical analyses
by showing that with statistically significant frequency, naive-Bayes classifiers trained on
data discretized by our new methods are able to achieve lower classification error than those
trained on data discretized by current established discretization methods.},
  doi       = {10.1007/s10994-008-5083-5},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/2009-Yang-ML.pdf:PDF},
  keywords  = {Discretization · Naive-Bayes Learning · Bias · Variance},
  publisher = {Springer},
}

@Article{dimitrova2010discretization,
  author    = {Dimitrova, Elena S and Licona, M Paola Vera and McGee, John and Laubenbacher, Reinhard},
  journal   = {Journal of Computational Biology},
  title     = {Discretization of time series data},
  year      = {2010},
  number    = {6},
  pages     = {853--868},
  volume    = {17},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/dimitrova-elena-s-discretization-of-time-series-data.pdf:PDF;:D\:/School/TESIS/bibliografia/automl, ml, md, data preprocessing/Ciencias_de_la_Ingenieria_y_Tecnologia_H.pdf:PDF},
  publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
}

@Article{berthold2009knime,
  author    = {Berthold, Michael R and Cebron, Nicolas and Dill, Fabian and Gabriel, Thomas R and K{\"o}tter, Tobias and Meinl, Thorsten and Ohl, Peter and Thiel, Kilian and Wiswedel, Bernd},
  journal   = {AcM SIGKDD explorations Newsletter},
  title     = {KNIME-the Konstanz information miner: version 2.0 and beyond},
  year      = {2009},
  number    = {1},
  pages     = {26--31},
  volume    = {11},
  publisher = {ACM New York, NY, USA},
}

@Article{mendez2007sistemas,
  author    = {M{\'e}ndez, Jos{\'e} R and Riverola, Florentino Fdez and D{\'\i}az, Fernando and Corchado, Juan M},
  journal   = {Inteligencia Artificial. Revista Iberoamericana de Inteligencia Artificial},
  title     = {Sistemas inteligentes para la detecci{\'o}n y filtrado de correo spam: una revisi{\'o}n},
  year      = {2007},
  number    = {34},
  pages     = {63--81},
  volume    = {11},
  file      = {:D\:/School/TESIS/bibliografia/automl, ml, md, data preprocessing/SSRN-id3349586.pdf:PDF},
  publisher = {Asociaci{\'o}n Espa{\~n}ola para la Inteligencia Artificial},
}

@Article{borras2017clasificacion,
  author  = {Borr{\`a}s, J and Delegido, Jes{\'u}s and Pezzola, A and Pereira-Sandoval, M and Morassi, G and Camps-Valls, G},
  journal = {Revista de Teledetecci{\'o}n},
  title   = {Clasificaci{\'o}n de usos del suelo a partir de im{\'a}genes Sentinel-2},
  year    = {2017},
  number  = {48},
  pages   = {55--66},
}

@InProceedings{dhankhad2018supervised,
  author       = {Dhankhad, Sahil and Mohammed, Emad and Far, Behrouz},
  booktitle    = {2018 IEEE international conference on information reuse and integration (IRI)},
  title        = {Supervised machine learning algorithms for credit card fraudulent transaction detection: a comparative study},
  year         = {2018},
  organization = {IEEE},
  pages        = {122--125},
}

@Book{gatignon2003statistical,
  author    = {Gatignon, Hubert},
  publisher = {Springer},
  title     = {Statistical analysis of management data},
  year      = {2003},
  file      = {:D\:/School/Informatica/4to Año/1er semestre/Practicas/bibliografia/hubert-gatignon-statistical-analysis-of-management.pdf:PDF},
}

@Article{sturges1926choice,
  author    = {Sturges, Herbert A},
  journal   = {Journal of the american statistical association},
  title     = {The choice of a class interval},
  year      = {1926},
  number    = {153},
  pages     = {65--66},
  volume    = {21},
  file      = {:D\:/School/TESIS/bibliografia/discretization/Sturges1926.pdf:PDF},
  publisher = {New York},
}

@Article{morelo2014sistema,
  author  = {Morelo, K},
  journal = {Cartagena de Indias: Universidad de Cartagena},
  title   = {Sistema para caracterizaci{\'o}n de perfiles de clientes de la empresa Zona T},
  year    = {2014},
  file    = {:D\:/School/TESIS/bibliografia/automl, ml, md, data preprocessing/Trabajo_de_Grado_MoreloTapias_Kenny_Alejandro.pdf:PDF},
}

@Article{coria2013mineria,
  author  = {Coria, Sergio and Orozco, Ivania and Luna, Juan},
  journal = {de Cuerpos Acad{\'e}micos},
  title   = {Minería de datos para perfilamiento de las brechas digital y educativa de ciudades en censos de poblaci{\'o}n y vivienda},
  year    = {2013},
  pages   = {18},
  file    = {:D\:/School/TESIS/bibliografia/automl, ml, md, data preprocessing/Ciencias_de_la_Ingenieria_y_Tecnologia_H.pdf:PDF},
}

@InProceedings{misra2019impact,
  author    = {Misra, Puneet and Yadav, Arun Singh},
  booktitle = {Proceedings of 2nd International Conference on Advanced Computing and Software Engineering (ICACSE)},
  title     = {Impact of preprocessing methods on healthcare predictions},
  year      = {2019},
}

@InProceedings{li2004towards,
  author       = {Li, Dan and Deogun, Jitender and Spaulding, William and Shuart, Bill},
  booktitle    = {Rough Sets and Current Trends in Computing: 4th International Conference, RSCTC 2004, Uppsala, Sweden, June 1-5, 2004. Proceedings 4},
  title        = {Towards missing data imputation: a study of fuzzy k-means clustering method},
  year         = {2004},
  organization = {Springer},
  pages        = {573--579},
  comment      = {kmeans imp},
  file         = {:D\:/School/TESIS/bibliografia/dealing with mv/dan-li-towards-missing-data-imputation-a-study-of.pdf:PDF},
}

@Article{tsai2022empirical,
  author    = {Tsai, Chih-Fong and Hu, Ya-Han},
  journal   = {Knowledge and Information Systems},
  title     = {Empirical comparison of supervised learning techniques for missing value imputation},
  year      = {2022},
  number    = {4},
  pages     = {1047--1075},
  volume    = {64},
  comment   = {knni},
  file      = {:D\:/School/TESIS/bibliografia/dealing with mv/chih-fong-tsai-empirical-comparison-of-supervised.pdf:PDF},
  keywords  = {Missing value · Imputation · Supervised learning · Incomplete dataset · Data mining · Data preprocessing},
  publisher = {Springer},
}

@InProceedings{patil2010missing,
  author       = {Patil, Bankat M and Joshi, Ramesh C and Toshniwal, Durga},
  booktitle    = {Contemporary Computing: Third International Conference, IC3 2010, Noida, India, August 9-11, 2010. Proceedings, Part I 3},
  title        = {Missing value imputation based on k-mean clustering with weighted distance},
  year         = {2010},
  organization = {Springer},
  pages        = {600--609},
}

@Article{batista2003analysis,
  author    = {Batista, Gustavo EAPA and Monard, Maria Carolina},
  journal   = {Applied artificial intelligence},
  title     = {An analysis of four missing data treatment methods for supervised learning},
  year      = {2003},
  number    = {5-6},
  pages     = {519--533},
  volume    = {17},
  file      = {:D\:/School/TESIS/bibliografia/dealing with mv/An analysis of four missing data treatment methods for supervised learning.pdf:PDF},
  publisher = {Taylor \& Francis},
}

@Article{avanzi2023machine,
  author   = {Avanzi, Benjamin and Taylor, Greg and Wang, Melantha and Wong, Bernard},
  journal  = {arXiv preprint arXiv:2301.12710},
  title    = {Machine Learning with High-Cardinality Categorical Features in Actuarial Applications},
  year     = {2023},
  file     = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/feature selection - cardinality reduction/2301.12710.pdf:PDF},
  keywords = {Categorical features; Generalised linear mixed models; Neural networks; Categorical embedding; Random effects; Variational inference; Insurance analytics JEL codes: C45, C51, C52, C53, G22 MSC classes: 91G70, 91G60, 62P},
}

@Article{domingos2012few,
  author    = {Domingos, Pedro},
  journal   = {Communications of the ACM},
  title     = {A few useful things to know about machine learning},
  year      = {2012},
  number    = {10},
  pages     = {78--87},
  volume    = {55},
  file      = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/feature selection - cardinality reduction/pedro-domingos-a-few-useful-things-to-know-about.pdf:PDF},
  publisher = {ACM New York, NY, USA},
}

@PhdThesis{Cerda2019,
  author   = {Patricio Cerda},
  school   = {Universit{\'e} Paris-Saclay},
  title    = {Statistical learning with high-cardinality string categorical variables},
  year     = {2019},
  abstract = {Computer Science [cs]/Machine Learning [cs.LG]},
  file     = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/feature selection - cardinality reduction/84377_CERDA_REYES_2019_archivage.pdf:PDF},
  keywords = {Statistical learning, Large-Scale data, String categorical variables, AutoMLApprentissage statistique, Données sales, Variables catégorielles, AutoML, Données volumineuses},
}

@Article{cerda2018similarity,
  author    = {Cerda, Patricio and Varoquaux, Ga{\"e}l and K{\'e}gl, Bal{\'a}zs},
  journal   = {Machine Learning},
  title     = {Similarity encoding for learning with dirty categorical variables},
  year      = {2018},
  number    = {8-10},
  pages     = {1477--1494},
  volume    = {107},
  file      = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/feature selection - cardinality reduction/article_hal.pdf:PDF},
  publisher = {Springer},
}

@Article{jose2014survey,
  author   = {Jose, Jesna},
  journal  = {International Journal of Engineering Research and General Science August – September},
  title    = {A survey on feature selection techniques},
  year     = {2014},
  number   = {5},
  pages    = {44--46},
  volume   = {2},
  abstract = {Feature selection is a term commonly used in data mining to describe the tools and techniques available for reducing},
  file     = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/feature selection - cardinality reduction/A-SURVEY-7.pdf:PDF},
  keywords = {Feature selection, Graph based clustering, Redundancy, Relevance, Minimum spanning tree, Symmetric uncertainity, correlation},
}

@Article{chandrashekar2014survey,
  author    = {Chandrashekar, Girish and Sahin, Ferat},
  journal   = {Computers \& Electrical Engineering},
  title     = {A survey on feature selection methods},
  year      = {2014},
  number    = {1},
  pages     = {16--28},
  volume    = {40},
  file      = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/feature selection - cardinality reduction/chandrashekar-girish-a-survey-on-feature-selection.pdf:PDF},
  publisher = {Elsevier},
}

@TechReport{hooi2022feature,
  author    = {Hooi, Eric Khoo Jiun and Zainal, Anazida and Kassim, Mohamad Nizam and Ayub, Zaily},
  title     = {Feature Encoding For High Cardinality Categorical Variables Using Entity Embeddings: A Case Study},
  year      = {2022},
  abstract  = {Customs authorities nowadays are pressurized detecting tax fraudsters as the traditional approaches cannot
by the increasing levels of international trade and insufficient work well in the current fast evolving environment. 
resources to perform physical check on all the trades.},
  booktitle = {2022 International Conference on Cyber Resilience (ICCR) |},
  file      = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/feature selection - cardinality reduction/eric-khoo-jiun-hooi-feature-encoding-for-high.pdf:PDF},
  keywords  = {High Cardinality, Entity Embeddings, Customs},
  pages     = {1--5},
  publisher = {IEEE},
}

@Article{vanhoeyveld2020customs,
  author    = {Vanhoeyveld, Jellis and Martens, David and Peeters, Bruno},
  journal   = {Pattern analysis and applications},
  title     = {Customs fraud detection: Assessing the value of behavioural and high-cardinality data under the imbalanced learning issue},
  year      = {2020},
  pages     = {1457--1477},
  volume    = {23},
  file      = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/feature selection - cardinality reduction/jellis-vanhoeyveld-customs-fraud-detection-assessing.pdf:PDF},
  keywords  = {Fraud detection · Behavioural data · High-cardinality attributes · Imbalanced learning · Support vector machines},
  publisher = {Springer},
}

@Article{moeyersoms2015including,
  author    = {Moeyersoms, Julie and Martens, David},
  journal   = {Decision support systems},
  title     = {Including high-cardinality attributes in predictive models: A case study in churn prediction in the energy sector},
  year      = {2015},
  pages     = {72--81},
  volume    = {72},
  file      = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/feature selection - cardinality reduction/julie-moeyersoms-including-high-cardinality-attributes.pdf:PDF},
  publisher = {Elsevier},
}

@MastersThesis{ventevogel2020construction,
  author = {Ventevogel, PC},
  school = {University of Twente},
  title  = {Construction of a proactive alert management model by using artificial intelligence},
  year   = {2020},
  file   = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/feature selection - cardinality reduction/Ventevogel_MA_BMS.pdf:PDF},
}

@Article{casas2019data,
  author  = {Casas, Pablo},
  journal = {Retrieved from: livebook. datascienceheroes. com},
  title   = {Data science live book},
  year    = {2019},
}

@Article{cerda2020encoding,
  author    = {Cerda, Patricio and Varoquaux, Ga{\"e}l},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  title     = {Encoding high-cardinality string categorical variables},
  year      = {2020},
  number    = {3},
  pages     = {1164--1176},
  volume    = {34},
  publisher = {IEEE},
}

@Book{montavon2012neural,
  author    = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve and M{\"u}ller, Klaus-Robert},
  publisher = {springer},
  title     = {Neural networks: tricks of the trade},
  year      = {2012},
  volume    = {7700},
}

@Book{bishop2006pattern,
  author    = {Bishop, Christopher M and Nasrabadi, Nasser M},
  publisher = {Springer},
  title     = {Pattern recognition and machine learning},
  year      = {2006},
  number    = {4},
  volume    = {4},
}

@Book{joachims2002learning,
  author    = {Joachims, Thorsten},
  publisher = {Springer Science \& Business Media},
  title     = {Learning to classify text using support vector machines},
  year      = {2002},
  volume    = {668},
}

@Book{scholkopf2018learning,
  author    = {Scholkopf, Bernhard and Smola, Alexander J},
  publisher = {MIT press},
  title     = {Learning with kernels: support vector machines, regularization, optimization, and beyond},
  year      = {2018},
}

@Book{lakshmanan2021practical,
  author    = {Lakshmanan, Valliappa and G{\"o}rner, Martin and Gillard, Ryan},
  publisher = {" O'Reilly Media, Inc."},
  title     = {Practical machine learning for computer vision},
  year      = {2021},
}

@Article{mahmood2022accurate,
  author    = {Mahmood, Jawad and Luo, Ming and Rehman, Mudassar},
  journal   = {The International Journal of Advanced Manufacturing Technology},
  title     = {An accurate detection of tool wear type in drilling process by applying PCA and one-hot encoding to SSA-BLSTM model},
  year      = {2022},
  pages     = {1--20},
  doi       = {.org/10.1007/s00170-021-08200-1},
  file      = {:D\:/School/TESIS/bibliografia/feature selection - cardinality reduction/jawad-mahmood-an-accurate-detection-of-tool-wear-type.pdf:PDF},
  keywords  = {Drilling · Tool wear · SSA-BLSTM · PCA},
  publisher = {Springer},
}

@InProceedings{kasemtaweechok2021large,
  author       = {Kasemtaweechok, Chatchai and Pharkdepinyo, Patnaree and Doungmanee, Patimakorn},
  booktitle    = {2021 Joint International Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunication Engineering},
  title        = {Large-Scale Instance Selection using Center of Principal Components},
  year         = {2021},
  organization = {IEEE},
  pages        = {157--160},
  file         = {:D\:/School/TESIS/bibliografia/feature selection - cardinality reduction/chatchai-kasemtaweechok-large-scale-instance-selection.pdf:PDF},
}

@Article{bhattacharya2020novel,
  author    = {Bhattacharya, Sweta and Maddikunta, Praveen Kumar Reddy and Kaluri, Rajesh and Singh, Saurabh and Gadekallu, Thippa Reddy and Alazab, Mamoun and Tariq, Usman},
  journal   = {Electronics},
  title     = {A novel PCA-firefly based XGBoost classification model for intrusion detection in networks using GPU},
  year      = {2020},
  number    = {2},
  pages     = {219},
  volume    = {9},
  doi       = {.org/10.1007/s11554-020-00987-8},
  file      = {:C\:/Users/Jenny/Downloads/Telegram Desktop/thippa-reddy-gadekallu-a-novel-pca-whale-optimization.pdf:PDF},
  keywords  = {Tomato leaf disease classification · Deep neural networks · Grid search hyperparameter tuning · Principal component analysis · Whale optimization algorithm},
  publisher = {MDPI},
}

@Article{patro2015normalization,
  author   = {Patro, SGOPAL and Sahu, Kishore Kumar},
  journal  = {arXiv preprint arXiv:1503.06462},
  title    = {Normalization: A preprocessing stage},
  year     = {2015},
  abstract = {As we know that the normalization is a pre-processing stage of any type problem statement. Especially
normalization takes important role in the field of soft computing, cloud computing etc. for manipulation of data like 
scale down or scale up the range of data before it becomes used for further stage. There are so many normalization 
techniques are there namely Min-Max normalization, Z-score normalization and Decimal scaling normalization. So by 
referring these normalization techniques we are going to propose one new normalization technique namely, Integer 
Scaling Normalization. And we are going to show our proposed normalization technique using various data sets. 
 
Keywords: Normalization, Scaling, Transformation, Integer Scaling, AMZD},
  file     = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/normalization/1503.06462.pdf:PDF},
}

@Article{polatgil2022investigation,
  author  = {Polatgil, Mesut},
  journal = {Int. J. Inf. Technol. Comput. Sci},
  title   = {Investigation of the effect of normalization methods on ANFIS success: forestfire and diabets datasets},
  year    = {2022},
  pages   = {1--8},
  volume  = {14},
  file    = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/normalization/IJITCS-V14-N1-1.pdf:PDF},
}

@Article{2021,
  author    = {Ahsan, Md and Mahmud, M. and Saha, Pritom and Gupta, Kishor and Siddique, Zahed},
  journal   = {Technologies},
  title     = {Effect of Data Scaling Methods on Machine Learning Algorithms and Model Performance},
  year      = {2021},
  issn      = {2227-7080},
  month     = {Jul},
  number    = {3},
  pages     = {52},
  volume    = {9},
  doi       = {10.3390/technologies9030052},
  file      = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/normalization/technologies-09-00052-v3.pdf:PDF},
  keywords  = {heart disease; machine learning algorithm; data scaling; prediction; automated model},
  publisher = {MDPI AG},
  url       = {http://dx.doi.org/10.3390/technologies9030052},
}

@Article{quinlan1993c,
  author  = {Quinlan, J Ross},
  journal = {The Morgan Kaufmann Series in Machine Learning},
  title   = {C 4.5: Programs for machine learning},
  year    = {1993},
}

@Article{holte1993very,
  author    = {Holte, Robert C},
  journal   = {Machine learning},
  title     = {Very simple classification rules perform well on most commonly used datasets},
  year      = {1993},
  pages     = {63--90},
  volume    = {11},
  publisher = {Springer},
}

@InProceedings{catlett1991changing,
  author       = {Catlett, Jason},
  booktitle    = {Machine Learning—EWSL-91: European Working Session on Learning Porto, Portugal, March 6--8, 1991 Proceedings 5},
  title        = {On changing continuous attributes into ordered discrete attributes},
  year         = {1991},
  organization = {Springer},
  pages        = {164--178},
}

@Article{liu1997feature,
  author    = {Liu, Huan and Setiono, Rudy},
  journal   = {IEEE Transactions on knowledge and Data Engineering},
  title     = {Feature selection via discretization},
  year      = {1997},
  number    = {4},
  pages     = {642--645},
  volume    = {9},
  publisher = {IEEE},
}

@InProceedings{ledell2020h2o,
  author       = {LeDell, Erin and Poirier, Sebastien},
  booktitle    = {Proceedings of the AutoML Workshop at ICML},
  title        = {H2o automl: Scalable automatic machine learning},
  year         = {2020},
  organization = {ICML},
  volume       = {2020},
  file         = {:C\:/Users/Jenny/Documents/TESIS/bibliografia/automl, misc/AutoML_2020_paper_61.pdf:PDF},
}

@Misc{KNIME2023-11,
  author  = {KNIME},
  month   = nov,
  note    = {Visitado el 5 de noviembre del 2023},
  title   = {AutoML KNIME},
  year    = {2023},
  comment = {Visitado el 5 de noviembre de 2023},
  journal = {KNIME Official Site},
  url     = {https://hub.knime.com/knime/spaces/Examples/00_Components/Automation/AutoML~33fQGaQzuZByy6hE/1},
}

@Book{davison1997bootstrap,
  publisher = {Cambridge university press},
  title     = {Bootstrap methods and their application},
  year      = {1997},
  number    = {1},
}

@Comment{jabref-meta: databaseType:bibtex;}
